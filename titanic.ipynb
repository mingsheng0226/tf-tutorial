{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the training data\n",
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean the data\n",
    "data['Sex'] =  data['Sex'].apply(lambda s: 1 if s == 'male' else 0)\n",
    "data = data.fillna(0)\n",
    "dataset_X = data[['Sex', 'Age', 'Pclass', 'SibSp', 'Parch', 'Fare']]\n",
    "dataset_X = dataset_X.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one-hot encoding the label\n",
    "data['Deceased'] = data['Survived'].apply(lambda s: int(not s))\n",
    "dataset_Y = data[['Deceased', 'Survived']]\n",
    "dataset_Y = dataset_Y.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split training and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    dataset_X, dataset_Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Dataflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# arguments that can be set in command line\n",
    "tf.app.flags.DEFINE_integer('epochs', 10, 'Training epochs')\n",
    "tf.app.flags.DEFINE_integer('batch_size', 10, 'size of mini-batch')\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('input'):\n",
    "    # create symbolic variables\n",
    "    # the None means we can put any number of records for training\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 6])\n",
    "    y_true = tf.placeholder(tf.float32, shape=[None, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('classifier'):\n",
    "    # weights and bias are the variables to be trained\n",
    "    weights = tf.Variable(tf.random_normal([6, 2]))\n",
    "    bias = tf.Variable(tf.zeros([2]))\n",
    "    \n",
    "    # softmax is the multiclass logistic regression\n",
    "    y_pred = tf.nn.softmax(tf.matmul(X, weights) + bias)\n",
    "\n",
    "    # add histogram summaries for weights, view on tensorboard\n",
    "    tf.summary.histogram('weights', weights)\n",
    "    tf.summary.histogram('bias', bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Minimise cost using cross entropy\n",
    "# NOTE: add a epsilon(1e-10) when calculate log(y_pred),\n",
    "# otherwise the result will be -inf\n",
    "with tf.name_scope('cost'):\n",
    "    cross_entropy = - tf.reduce_sum(y_true * tf.log(y_pred + 1e-10),\n",
    "                                    reduction_indices=1)\n",
    "    cost = tf.reduce_mean(cross_entropy)\n",
    "    tf.summary.scalar('loss', cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use gradient descent optimizer to minimize cost\n",
    "with tf.name_scope('optimizer'):\n",
    "    train_op = tf.train.GradientDescentOptimizer(0.001).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    correct_pred = tf.equal(tf.argmax(y_true, 1), tf.argmax(y_pred, 1))\n",
    "    acc_op = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    # Add scalar summary for accuracy\n",
    "    tf.summary.scalar('accuracy', acc_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, name='global_step', trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use saver to save and restore model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# this variable won't be stored, since it is declared after tf.train.Saver()\n",
    "non_storable_variable = tf.Variable(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ckpt_dir = './ckpt_dir'\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use session to run the calculation\n",
    "with tf.Session() as sess:\n",
    "    # create a log writer. run 'tensorboard --logdir=./logs'\n",
    "    writer = tf.summary.FileWriter('./logs', sess.graph)\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    # variables have to be initialized at the first place\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    # restore variables from 'checkpoint' file if exists ('checkpoint' stores the latest saved checkpoint)\n",
    "    ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        print('Restoring from checkpoint: %s' % ckpt.model_checkpoint_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "    # get the last global step, t.eval() same as tf.get_default_session().run(t), eval the tensor\n",
    "    start = global_step.eval()\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(start, start + FLAGS.epochs):\n",
    "        total_loss = 0.\n",
    "        for i in range(0, len(X_train), FLAGS.batch_size):\n",
    "            # train with mini-batch\n",
    "            feed_dict = {\n",
    "                X: X_train[i: i + FLAGS.batch_size],\n",
    "                y_true: y_train[i: i + FLAGS.batch_size]\n",
    "            }\n",
    "            # fetches and feed_dict\n",
    "            _, loss = sess.run([train_op, cost], feed_dict=feed_dict)\n",
    "            total_loss += loss\n",
    "        # display loss per epoch\n",
    "        print('Epoch: %04d, loss=%.9f' % (epoch + 1, total_loss))\n",
    "\n",
    "        summary, accuracy = sess.run([merged, acc_op],\n",
    "                                     feed_dict={X: X_val, y_true: y_val})\n",
    "        writer.add_summary(summary, epoch)  # Write summary\n",
    "        print('Accuracy on validation set: %.9f' % accuracy)\n",
    "\n",
    "        # set and update(eval) global_step with epoch\n",
    "        global_step.assign(epoch).eval()\n",
    "        saver.save(sess, ckpt_dir + '/logistic.ckpt',\n",
    "                   global_step=global_step)\n",
    "    print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check the parameters in checkpoint\n",
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "print_tensors_in_checkpoint_file(file_name='./ckpt_dir/logistic.ckpt-5', tensor_name='', all_tensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Operation 'input/Placeholder' type=Placeholder>, <tf.Operation 'input/Placeholder_1' type=Placeholder>, <tf.Operation 'classifier/random_normal/shape' type=Const>, <tf.Operation 'classifier/random_normal/mean' type=Const>, <tf.Operation 'classifier/random_normal/stddev' type=Const>, <tf.Operation 'classifier/random_normal/RandomStandardNormal' type=RandomStandardNormal>, <tf.Operation 'classifier/random_normal/mul' type=Mul>, <tf.Operation 'classifier/random_normal' type=Add>, <tf.Operation 'classifier/Variable' type=VariableV2>, <tf.Operation 'classifier/Variable/Assign' type=Assign>, <tf.Operation 'classifier/Variable/read' type=Identity>, <tf.Operation 'classifier/zeros' type=Const>, <tf.Operation 'classifier/Variable_1' type=VariableV2>, <tf.Operation 'classifier/Variable_1/Assign' type=Assign>, <tf.Operation 'classifier/Variable_1/read' type=Identity>, <tf.Operation 'classifier/MatMul' type=MatMul>, <tf.Operation 'classifier/add' type=Add>, <tf.Operation 'classifier/Softmax' type=Softmax>, <tf.Operation 'classifier/weights/tag' type=Const>, <tf.Operation 'classifier/weights' type=HistogramSummary>, <tf.Operation 'classifier/bias/tag' type=Const>, <tf.Operation 'classifier/bias' type=HistogramSummary>, <tf.Operation 'cost/add/y' type=Const>, <tf.Operation 'cost/add' type=Add>, <tf.Operation 'cost/Log' type=Log>, <tf.Operation 'cost/mul' type=Mul>, <tf.Operation 'cost/Sum/reduction_indices' type=Const>, <tf.Operation 'cost/Sum' type=Sum>, <tf.Operation 'cost/Neg' type=Neg>, <tf.Operation 'cost/Const' type=Const>, <tf.Operation 'cost/Mean' type=Mean>, <tf.Operation 'cost/loss/tags' type=Const>, <tf.Operation 'cost/loss' type=ScalarSummary>, <tf.Operation 'optimizer/gradients/Shape' type=Const>, <tf.Operation 'optimizer/gradients/Const' type=Const>, <tf.Operation 'optimizer/gradients/Fill' type=Fill>, <tf.Operation 'optimizer/gradients/cost/Mean_grad/Reshape/shape' type=Const>, <tf.Operation 'optimizer/gradients/cost/Mean_grad/Reshape' type=Reshape>, <tf.Operation 'optimizer/gradients/cost/Mean_grad/Shape' type=Shape>, <tf.Operation 'optimizer/gradients/cost/Mean_grad/Tile' type=Tile>, <tf.Operation 'optimizer/gradients/cost/Mean_grad/Shape_1' type=Shape>, <tf.Operation 'optimizer/gradients/cost/Mean_grad/Shape_2' type=Const>, <tf.Operation 'optimizer/gradients/cost/Mean_grad/Const' type=Const>, <tf.Operation 'optimizer/gradients/cost/Mean_grad/Prod' type=Prod>, <tf.Operation 'optimizer/gradients/cost/Mean_grad/Const_1' type=Const>, <tf.Operation 'optimizer/gradients/cost/Mean_grad/Prod_1' type=Prod>, <tf.Operation 'optimizer/gradients/cost/Mean_grad/Maximum/y' type=Const>, <tf.Operation 'optimizer/gradients/cost/Mean_grad/Maximum' type=Maximum>, <tf.Operation 'optimizer/gradients/cost/Mean_grad/floordiv' type=FloorDiv>, <tf.Operation 'optimizer/gradients/cost/Mean_grad/Cast' type=Cast>, <tf.Operation 'optimizer/gradients/cost/Mean_grad/truediv' type=RealDiv>, <tf.Operation 'optimizer/gradients/cost/Neg_grad/Neg' type=Neg>, <tf.Operation 'optimizer/gradients/cost/Sum_grad/Shape' type=Shape>, <tf.Operation 'optimizer/gradients/cost/Sum_grad/Size' type=Const>, <tf.Operation 'optimizer/gradients/cost/Sum_grad/add' type=Add>, <tf.Operation 'optimizer/gradients/cost/Sum_grad/mod' type=FloorMod>, <tf.Operation 'optimizer/gradients/cost/Sum_grad/Shape_1' type=Const>, <tf.Operation 'optimizer/gradients/cost/Sum_grad/range/start' type=Const>, <tf.Operation 'optimizer/gradients/cost/Sum_grad/range/delta' type=Const>, <tf.Operation 'optimizer/gradients/cost/Sum_grad/range' type=Range>, <tf.Operation 'optimizer/gradients/cost/Sum_grad/Fill/value' type=Const>, <tf.Operation 'optimizer/gradients/cost/Sum_grad/Fill' type=Fill>, <tf.Operation 'optimizer/gradients/cost/Sum_grad/DynamicStitch' type=DynamicStitch>, <tf.Operation 'optimizer/gradients/cost/Sum_grad/Maximum/y' type=Const>, <tf.Operation 'optimizer/gradients/cost/Sum_grad/Maximum' type=Maximum>, <tf.Operation 'optimizer/gradients/cost/Sum_grad/floordiv' type=FloorDiv>, <tf.Operation 'optimizer/gradients/cost/Sum_grad/Reshape' type=Reshape>, <tf.Operation 'optimizer/gradients/cost/Sum_grad/Tile' type=Tile>, <tf.Operation 'optimizer/gradients/cost/mul_grad/Shape' type=Shape>, <tf.Operation 'optimizer/gradients/cost/mul_grad/Shape_1' type=Shape>, <tf.Operation 'optimizer/gradients/cost/mul_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>, <tf.Operation 'optimizer/gradients/cost/mul_grad/mul' type=Mul>, <tf.Operation 'optimizer/gradients/cost/mul_grad/Sum' type=Sum>, <tf.Operation 'optimizer/gradients/cost/mul_grad/Reshape' type=Reshape>, <tf.Operation 'optimizer/gradients/cost/mul_grad/mul_1' type=Mul>, <tf.Operation 'optimizer/gradients/cost/mul_grad/Sum_1' type=Sum>, <tf.Operation 'optimizer/gradients/cost/mul_grad/Reshape_1' type=Reshape>, <tf.Operation 'optimizer/gradients/cost/mul_grad/tuple/group_deps' type=NoOp>, <tf.Operation 'optimizer/gradients/cost/mul_grad/tuple/control_dependency' type=Identity>, <tf.Operation 'optimizer/gradients/cost/mul_grad/tuple/control_dependency_1' type=Identity>, <tf.Operation 'optimizer/gradients/cost/Log_grad/Reciprocal' type=Reciprocal>, <tf.Operation 'optimizer/gradients/cost/Log_grad/mul' type=Mul>, <tf.Operation 'optimizer/gradients/cost/add_grad/Shape' type=Shape>, <tf.Operation 'optimizer/gradients/cost/add_grad/Shape_1' type=Const>, <tf.Operation 'optimizer/gradients/cost/add_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>, <tf.Operation 'optimizer/gradients/cost/add_grad/Sum' type=Sum>, <tf.Operation 'optimizer/gradients/cost/add_grad/Reshape' type=Reshape>, <tf.Operation 'optimizer/gradients/cost/add_grad/Sum_1' type=Sum>, <tf.Operation 'optimizer/gradients/cost/add_grad/Reshape_1' type=Reshape>, <tf.Operation 'optimizer/gradients/cost/add_grad/tuple/group_deps' type=NoOp>, <tf.Operation 'optimizer/gradients/cost/add_grad/tuple/control_dependency' type=Identity>, <tf.Operation 'optimizer/gradients/cost/add_grad/tuple/control_dependency_1' type=Identity>, <tf.Operation 'optimizer/gradients/classifier/Softmax_grad/mul' type=Mul>, <tf.Operation 'optimizer/gradients/classifier/Softmax_grad/Sum/reduction_indices' type=Const>, <tf.Operation 'optimizer/gradients/classifier/Softmax_grad/Sum' type=Sum>, <tf.Operation 'optimizer/gradients/classifier/Softmax_grad/Reshape/shape' type=Const>, <tf.Operation 'optimizer/gradients/classifier/Softmax_grad/Reshape' type=Reshape>, <tf.Operation 'optimizer/gradients/classifier/Softmax_grad/sub' type=Sub>, <tf.Operation 'optimizer/gradients/classifier/Softmax_grad/mul_1' type=Mul>, <tf.Operation 'optimizer/gradients/classifier/add_grad/Shape' type=Shape>, <tf.Operation 'optimizer/gradients/classifier/add_grad/Shape_1' type=Const>, <tf.Operation 'optimizer/gradients/classifier/add_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>, <tf.Operation 'optimizer/gradients/classifier/add_grad/Sum' type=Sum>, <tf.Operation 'optimizer/gradients/classifier/add_grad/Reshape' type=Reshape>, <tf.Operation 'optimizer/gradients/classifier/add_grad/Sum_1' type=Sum>, <tf.Operation 'optimizer/gradients/classifier/add_grad/Reshape_1' type=Reshape>, <tf.Operation 'optimizer/gradients/classifier/add_grad/tuple/group_deps' type=NoOp>, <tf.Operation 'optimizer/gradients/classifier/add_grad/tuple/control_dependency' type=Identity>, <tf.Operation 'optimizer/gradients/classifier/add_grad/tuple/control_dependency_1' type=Identity>, <tf.Operation 'optimizer/gradients/classifier/MatMul_grad/MatMul' type=MatMul>, <tf.Operation 'optimizer/gradients/classifier/MatMul_grad/MatMul_1' type=MatMul>, <tf.Operation 'optimizer/gradients/classifier/MatMul_grad/tuple/group_deps' type=NoOp>, <tf.Operation 'optimizer/gradients/classifier/MatMul_grad/tuple/control_dependency' type=Identity>, <tf.Operation 'optimizer/gradients/classifier/MatMul_grad/tuple/control_dependency_1' type=Identity>, <tf.Operation 'optimizer/GradientDescent/learning_rate' type=Const>, <tf.Operation 'optimizer/GradientDescent/update_classifier/Variable/ApplyGradientDescent' type=ApplyGradientDescent>, <tf.Operation 'optimizer/GradientDescent/update_classifier/Variable_1/ApplyGradientDescent' type=ApplyGradientDescent>, <tf.Operation 'optimizer/GradientDescent' type=NoOp>, <tf.Operation 'accuracy/ArgMax/dimension' type=Const>, <tf.Operation 'accuracy/ArgMax' type=ArgMax>, <tf.Operation 'accuracy/ArgMax_1/dimension' type=Const>, <tf.Operation 'accuracy/ArgMax_1' type=ArgMax>, <tf.Operation 'accuracy/Equal' type=Equal>, <tf.Operation 'accuracy/Cast' type=Cast>, <tf.Operation 'accuracy/Const' type=Const>, <tf.Operation 'accuracy/Mean' type=Mean>, <tf.Operation 'accuracy/accuracy/tags' type=Const>, <tf.Operation 'accuracy/accuracy' type=ScalarSummary>, <tf.Operation 'global_step/initial_value' type=Const>, <tf.Operation 'global_step' type=VariableV2>, <tf.Operation 'global_step/Assign' type=Assign>, <tf.Operation 'global_step/read' type=Identity>, <tf.Operation 'save/Const' type=Const>, <tf.Operation 'save/SaveV2/tensor_names' type=Const>, <tf.Operation 'save/SaveV2/shape_and_slices' type=Const>, <tf.Operation 'save/SaveV2' type=SaveV2>, <tf.Operation 'save/control_dependency' type=Identity>, <tf.Operation 'save/RestoreV2/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2' type=RestoreV2>, <tf.Operation 'save/Assign' type=Assign>, <tf.Operation 'save/RestoreV2_1/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_1/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_1' type=RestoreV2>, <tf.Operation 'save/Assign_1' type=Assign>, <tf.Operation 'save/RestoreV2_2/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_2/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_2' type=RestoreV2>, <tf.Operation 'save/Assign_2' type=Assign>, <tf.Operation 'save/restore_all' type=NoOp>, <tf.Operation 'Variable/initial_value' type=Const>, <tf.Operation 'Variable' type=VariableV2>, <tf.Operation 'Variable/Assign' type=Assign>, <tf.Operation 'Variable/read' type=Identity>, <tf.Operation 'Merge/MergeSummary' type=MergeSummary>, <tf.Operation 'init' type=NoOp>]\n"
     ]
    }
   ],
   "source": [
    "# check the nodes in dataflow graph\n",
    "g = tf.get_default_graph()\n",
    "print g.get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
